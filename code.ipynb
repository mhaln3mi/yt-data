{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authorname</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>louisdavid</td>\n",
       "      <td>I just wanted to tell you that you are an exce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Susan McBride</td>\n",
       "      <td>Hey man. This worked incredibly well but when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L. Barrera</td>\n",
       "      <td>Hello and thanks again. In the example, you me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>georgebalbino</td>\n",
       "      <td>Great video and super clear explanation. Thank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TJ</td>\n",
       "      <td>Great video! Was looking to learn more about Y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      authorname                                           Comments\n",
       "0     louisdavid  I just wanted to tell you that you are an exce...\n",
       "1  Susan McBride  Hey man. This worked incredibly well but when ...\n",
       "2     L. Barrera  Hello and thanks again. In the example, you me...\n",
       "3  georgebalbino  Great video and super clear explanation. Thank...\n",
       "4             TJ  Great video! Was looking to learn more about Y..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ytdf.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45 entries, 0 to 44\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   authorname  45 non-null     object\n",
      " 1   Comments    45 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 848.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove emojis and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleaning_comments(comment):\n",
    "  comment = re.sub(\"[🤣|🤭|🤣|😁|🤭|❤️|👍|🏴|😣|😠|💪|🙏|😞|🌺|🌸|🌞|🌻|💐|💓|😥|💔|😪|😑|🏽|😢|😑|😇|💜|🪴|🙌🏻|🇨🇦|🕊|🕯|😭|😔|💙|🏼|✝|🇿]+\",'',comment)\n",
    "  comment = re.sub(\"[0-9]+\",\"\",comment)\n",
    "  comment = re.sub(\"[\\:|\\@|\\)|\\*|\\.|\\$|\\!|\\?|\\,|\\%|\\\"|\\(|\\-|\\”|\\#|\\!|\\/|\\«|\\»|\\&|\\n|\\'|\\;|\\!|<|>|\\'|\\’|\\\\\\\\]+\",\" \",comment)\n",
    "  return comment\n",
    "\n",
    "df['Comments'] = df['Comments'].apply(cleaning_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = lambda x: x.lower()\n",
    "df['Comments'] = df['Comments'].apply(lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the polarity with the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\wled3/nltk_data'\n    - 'c:\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\wled3\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\wled3\\OneDrive\\سطح المكتب\\yt-data\\code.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wled3/OneDrive/%D8%B3%D8%B7%D8%AD%20%D8%A7%D9%84%D9%85%D9%83%D8%AA%D8%A8/yt-data/code.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msentiment\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvader\u001b[39;00m \u001b[39mimport\u001b[39;00m SentimentIntensityAnalyzer\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wled3/OneDrive/%D8%B3%D8%B7%D8%AD%20%D8%A7%D9%84%D9%85%D9%83%D8%AA%D8%A8/yt-data/code.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sid \u001b[39m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wled3/OneDrive/%D8%B3%D8%B7%D8%AD%20%D8%A7%D9%84%D9%85%D9%83%D8%AA%D8%A8/yt-data/code.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#Create scores col\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wled3/OneDrive/%D8%B3%D8%B7%D8%AD%20%D8%A7%D9%84%D9%85%D9%83%D8%AA%D8%A8/yt-data/code.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mScores\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mComments\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: sid\u001b[39m.\u001b[39mpolarity_scores(x))\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\nltk\\sentiment\\vader.py:340\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.__init__\u001b[1;34m(self, lexicon_file)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    337\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    338\u001b[0m     lexicon_file\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    339\u001b[0m ):\n\u001b[1;32m--> 340\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlexicon_file \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mload(lexicon_file)\n\u001b[0;32m    341\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlexicon \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_lex_dict()\n\u001b[0;32m    342\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstants \u001b[39m=\u001b[39m VaderConstants()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\wled3/nltk_data'\n    - 'c:\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\wled3\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "#Create scores col\n",
    "df['Scores'] = df['Comments'].apply(lambda x: sid.polarity_scores(x))\n",
    "\n",
    "#Create compund score\n",
    "df['Compound'] = df['Scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "\n",
    "#label based on score\n",
    "df['Polarity'] = df['Compound'].apply(lambda c: 'neutral' if -0.05 < c < 0.05 else ('negative' if c < -0.05 else 'positive'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the polarity without the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#Create comments without stopwords col\n",
    "df['comments wo stopwords'] = df['Comments'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add scores and polarity col without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\wled3/nltk_data'\n    - 'c:\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\wled3\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\wled3\\OneDrive\\سطح المكتب\\yt-data\\code.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wled3/OneDrive/%D8%B3%D8%B7%D8%AD%20%D8%A7%D9%84%D9%85%D9%83%D8%AA%D8%A8/yt-data/code.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msentiment\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvader\u001b[39;00m \u001b[39mimport\u001b[39;00m SentimentIntensityAnalyzer\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wled3/OneDrive/%D8%B3%D8%B7%D8%AD%20%D8%A7%D9%84%D9%85%D9%83%D8%AA%D8%A8/yt-data/code.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sid \u001b[39m=\u001b[39m SentimentIntensityAnalyzer()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wled3/OneDrive/%D8%B3%D8%B7%D8%AD%20%D8%A7%D9%84%D9%85%D9%83%D8%AA%D8%A8/yt-data/code.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mScores wo stopwords\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mcomments wo stopwords\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: sid\u001b[39m.\u001b[39mpolarity_scores(x))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wled3/OneDrive/%D8%B3%D8%B7%D8%AD%20%D8%A7%D9%84%D9%85%D9%83%D8%AA%D8%A8/yt-data/code.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mcompound wo stopwords\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mScores wo stopwords\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m score_dict: score_dict[\u001b[39m'\u001b[39m\u001b[39mcompound\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\nltk\\sentiment\\vader.py:340\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.__init__\u001b[1;34m(self, lexicon_file)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    337\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    338\u001b[0m     lexicon_file\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    339\u001b[0m ):\n\u001b[1;32m--> 340\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlexicon_file \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mload(lexicon_file)\n\u001b[0;32m    341\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlexicon \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_lex_dict()\n\u001b[0;32m    342\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstants \u001b[39m=\u001b[39m VaderConstants()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\wled3/nltk_data'\n    - 'c:\\\\Python310\\\\nltk_data'\n    - 'c:\\\\Python310\\\\share\\\\nltk_data'\n    - 'c:\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\wled3\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "df['Scores wo stopwords'] = df['comments wo stopwords'].apply(lambda x: sid.polarity_scores(x))\n",
    "\n",
    "df['compound wo stopwords'] = df['Scores wo stopwords'].apply(lambda score_dict: score_dict['compound'])\n",
    "\n",
    "df['polarity wo stopwords'] = df['compound wo stopwords'].apply(lambda c: 'neutral' if -0.05 < c < 0.05 else ('negative' if c < -0.05 else 'positive'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authorname</th>\n",
       "      <th>Comments</th>\n",
       "      <th>comments wo stopwords</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>louisdavid</td>\n",
       "      <td>i just wanted to tell you that you are an exce...</td>\n",
       "      <td>wanted tell excellent popularizer explain clea...</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Susan McBride</td>\n",
       "      <td>hey man  this worked incredibly well but when ...</td>\n",
       "      <td>hey man worked incredibly well used python vid...</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L. Barrera</td>\n",
       "      <td>hello and thanks again  in the example  you me...</td>\n",
       "      <td>hello thanks example mentioned grabbed slightl...</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>georgebalbino</td>\n",
       "      <td>great video and super clear explanation  thank...</td>\n",
       "      <td>great video super clear explanation thank sharing</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TJ</td>\n",
       "      <td>great video  was looking to learn more about y...</td>\n",
       "      <td>great video looking learn youtube comment scra...</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      authorname                                           Comments  \\\n",
       "0     louisdavid  i just wanted to tell you that you are an exce...   \n",
       "1  Susan McBride  hey man  this worked incredibly well but when ...   \n",
       "2     L. Barrera  hello and thanks again  in the example  you me...   \n",
       "3  georgebalbino  great video and super clear explanation  thank...   \n",
       "4             TJ  great video  was looking to learn more about y...   \n",
       "\n",
       "                               comments wo stopwords  Length  \n",
       "0  wanted tell excellent popularizer explain clea...     123  \n",
       "1  hey man worked incredibly well used python vid...     238  \n",
       "2  hello thanks example mentioned grabbed slightl...     389  \n",
       "3  great video super clear explanation thank sharing      63  \n",
       "4  great video looking learn youtube comment scra...     168  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Length\"] = df[\"Comments\"].str.len()\n",
    "df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
